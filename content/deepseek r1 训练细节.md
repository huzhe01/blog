
DeepSeek-R1 的核心创新在于使用了 **GRPO (Group Relative Policy Optimization)** 算法。为了回答你的问题，我将根据文献 和 中提供的数学公式，拆解其具体含义和每个运算项的作用。

GRPO 的核心目标函数 $J_{GRPO}(\theta)$ 如下所示：
$$J_{GRPO}(\theta) = \mathbb{E}[q \sim P(Q), {o_i}_{i=1}^G \sim \pi_{\theta_{old}}(O|q)] \frac{1}{G} \sum_{i=1}^G \left( \text{ppo\_clip\_loss}_i - \beta D_{KL}(\pi_{\theta} || \pi_{ref}) \right)$$

其中，最具特色的“分组相对优势” ($A_i$) 计算公式为： $$ A_i = \frac{r_i - \text{mean}({r_1, \dots, r_G})}{\text{std}({r_1, \dots, r_G})} $$
以下是每个部分的详细解释：

### 1. 采样与期望 $\mathbb{E}[q \sim P(Q), {o_i}_{i=1}^G \sim \pi_{\theta_{old}}(O|q)]$

- **含义：** 这部分描述了数据的来源。
- **解释：**
    - $q$：从题目数据集 $P(Q)$ 中抽取一个问题（Question）。
    - ${o_i}_{i=1}^G$：对于这**同一个问题** $q$，使用旧的策略模型 $\pi_{\theta_{old}}$ 生成 **一组（Group）** 共 $G$ 个不同的回答（Output）。例如，DeepSeek 在训练中设置 $G=16$ 或更大。
    - **关键点：** 传统的 PPO 通常只生成一个回答就进行评估，而 GRPO 必须生成一组回答，以便进行内部比较。

### 2. 优势函数 $A_i$ (Advantage)

这是 GRPO 算法名称中 "Group Relative"（分组相对）的来源，也是它**不需要价值模型（Critic Model）**的核心原因。

- **公式：** $A_i = \frac{r_i - \text{mean}({r_1, \dots, r_G})}{\text{std}({r_1, \dots, r_G})}$
- **解释：**
    - $r_i$：第 $i$ 个回答的原始得分（Reward）。如果回答正确可能是 1，错误是 0，或者包含格式分。
    - **Baseline（基准线）：** 传统的 RL 需要一个神经网络（Critic）来预测“这个题目大概能得多少分”作为基准。GRPO 不用预测，它直接计算**这一组回答的平均分** ($\text{mean}$) 作为基准。
    - **相对优势：** 既然大家都是同一个模型对同一个问题生成的回答，那么**得分高于平均分**的回答就是“好回答”（$A_i > 0$），**低于平均分**的就是“差回答”（$A_i < 0$）。
    - **标准化：** 除以标准差 ($\text{std}$) 是为了让优势值的尺度统一，便于训练稳定。

### 3. 策略比率 $\frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{old}}(o_i | q)}$

- **含义：** 衡量新模型与旧模型在生成同一个回答时的概率变化。
- **解释：**
    - 如果在参数更新后，新模型 $\pi_{\theta}$ 生成这个回答 $o_i$ 的概率变大了，这个比值就会大于 1。
    - 我们希望：对于优势 $A_i$ 为正（好）的回答，这个比率越大越好（鼓励生成）；对于优势 $A_i$ 为负（差）的回答，这个比率越小越好（抑制生成）。

### 4. 截断机制 (Clipping)

- **公式：** $\min(\dots, \text{clip}(\dots, 1-\epsilon, 1+\epsilon)A_i)$
- **含义：** 防止模型一次更新步子迈得太大，这是继承自 PPO 的核心机制。
- **解释：**
    - 如果不加限制，模型可能会为了提高某一个好回答的概率而剧烈修改参数，导致模型崩溃。
    - **Clip：** 强制将策略比率限制在 $[1-\epsilon, 1+\epsilon]$ 的范围内（例如 $\epsilon=0.2$）。这意味着我们只允许新旧模型的差异在一个小的区间内波动，确保训练的稳定性。

### 5. KL 散度惩罚 $-\beta D_{KL}(\pi_{\theta} || \pi_{ref})$

- **含义：** 这是一个正则化项，用于防止模型“忘本”或“玩坏”。
- **解释：**
    - $\pi_{ref}$：参考模型（Reference Model），通常是强化学习开始前的 SFT 模型。
    - $D_{KL}$：计算当前训练模型 $\pi_{\theta}$ 和参考模型 $\pi_{ref}$ 之间的分布差异。
    - **作用：** 如果训练出的模型为了拿高分，说话方式变得完全不像人类语言（例如乱码但能通过编译器），KL 散度就会变大，导致总奖励 $J$ 变小。这一项强迫模型在探索解题技巧的同时，保持原始的语言能力和风格。
    - **DeepSeek 的特殊处理：** 这里的 KL 是直接加在损失函数里的，而不是像传统 PPO 那样作为奖励的一部分扣除，这有助于避免对长思维链（Long CoT）的过度惩罚，鼓励模型多“思考”。

### 总结

用一个通俗的比喻来解释这个公式：

**GRPO 就像是一场“小组测验”：**

1. **采样：** 让模型针对同一个问题写 **16 个不同的答案**（一个小组）。
2. **优势 ($A_i$)：** 老师不给绝对分数，而是看**谁比组内的平均水平好**。如果全组都考得差，但你考得稍微好一点，你就是优秀的（优势为正）；如果全组都考满分，你只考了90分，那你就是差的（优势为负）。**不需要请一个专门的老师（Critic Model）来预估分数，大家互相比就行。**
3. **策略更新与截断：** 根据优势调整模型，让“优等生”出现的概率变大，但一次不能变太大（Clip），防止为了某一道题而彻底改变性格。
4. **KL 惩罚：** 无论怎么改，写出来的东西必须还得像人话（接近参考模型），不能为了解题变成乱码。

根据 DeepSeek-R1 的技术报告，其 **RL 基础设施（RL Infrastructure）** 被设计为一个**解耦（Decoupled）且高度可扩展**的系统，旨在解决超大规模模型（如 671B 参数的 MoE 模型）进行强化学习时的资源瓶颈和效率问题。

该架构主要由四个独立的模块组成，通过精细的显存管理和并行策略协同工作：

### 1. 四大核心模块 (The Four Modules)

整个 RL 流程被拆分为以下四个独立环节，各司其职：

| - **生成模块 (Rollout Module)**             | **功能：** 负责加载“演员模型”（Actor Model），根据提示词（Prompts）生成多个回答样本。<br>**技术细节：** 使用 **vLLM** 进行推理加速。针对 DeepSeek-V3 的 MoE（混合专家）架构，采用了**专家并行（Expert Parallelism）**策略以减少通信开销，并对“热点专家”（Hotspot Experts）进行冗余部署以以此平衡负载。此外，还利用了**多 Token 预测（MTP）**技术进行自投机解码（Self-Speculative Decoding），显著加快了长思维链的生成速度。 |
| --------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| - **推理/评估模块 (Inference Module)**        | - **功能：** 加载“参考模型”（Reference Model）和“奖励模型”（Reward Model），对生成模块产生的样本进行前向传播（Forward Pass）。<br>**目的：** 获取用于计算 KL 散度的参考概率分布，以及模型给出的奖励分数。                                                                                                                                                |
| - **规则奖励模块 (Rule-based Reward Module)** | **功能：** 计算基于规则的奖励（主要针对数学和代码任务）。<br>    - **特点：** 这是一个统一的接口，包含代码执行器（Code Executor）、答案匹配器（Answer Matcher）和格式检查器。<br>    - **优化：** 虽然该模块不需要 GPU，但计算耗时。系统采用了**异步调度（Asynchronous Scheduling）**，将其执行时间与生成和推理模块重叠，从而“隐藏”了这部分的延迟。                                                           |
| **训练模块 (Training Module)**              | **功能：** 加载 Actor 模型（在使用 PPO 时还需加载 Critic 模型），计算损失函数并更新参数。支持 GRPO、PPO 等多种算法。<br>**优化：**<br>**数据打包（Data Packing）：** 为了减少 Padding 造成的计算浪费，系统先对数据按长度排序，再使用“最佳适应”（Best-Fit）策略将数据打包成固定长度的块。<br>**并行策略：** 沿用了 DeepSeek-V3 预训练时的 **DualPipe** 算法来实现高效的流水线并行。                                |


- **生成模块 (Rollout Module)**
    - **功能：** 负责加载“演员模型”（Actor Model），根据提示词（Prompts）生成多个回答样本。
    - **技术细节：** 使用 **vLLM** 进行推理加速。针对 DeepSeek-V3 的 MoE（混合专家）架构，采用了**专家并行（Expert Parallelism）**策略以减少通信开销，并对“热点专家”（Hotspot Experts）进行冗余部署以以此平衡负载。此外，还利用了**多 Token 预测（MTP）**技术进行自投机解码（Self-Speculative Decoding），显著加快了长思维链的生成速度。
- **推理/评估模块 (Inference Module)**
    - **功能：** 加载“参考模型”（Reference Model）和“奖励模型”（Reward Model），对生成模块产生的样本进行前向传播（Forward Pass）。
    - **目的：** 获取用于计算 KL 散度的参考概率分布，以及模型给出的奖励分数。
- **规则奖励模块 (Rule-based Reward Module)**
    - **功能：** 计算基于规则的奖励（主要针对数学和代码任务）。
    - **特点：** 这是一个统一的接口，包含代码执行器（Code Executor）、答案匹配器（Answer Matcher）和格式检查器。
    - **优化：** 虽然该模块不需要 GPU，但计算耗时。系统采用了**异步调度（Asynchronous Scheduling）**，将其执行时间与生成和推理模块重叠，从而“隐藏”了这部分的延迟。
- **训练模块 (Training Module)**
    - **功能：** 加载 Actor 模型（在使用 PPO 时还需加载 Critic 模型），计算损失函数并更新参数。支持 GRPO、PPO 等多种算法。
    - **优化：**
        - **数据打包（Data Packing）：** 为了减少 Padding 造成的计算浪费，系统先对数据按长度排序，再使用“最佳适应”（Best-Fit）策略将数据打包成固定长度的块。
        - **并行策略：** 沿用了 DeepSeek-V3 预训练时的 **DualPipe** 算法来实现高效的流水线并行。

### 2. 关键优化机制

为了在有限的资源下训练巨型模型，该基础设施引入了两个关键的系统级优化：

- **显存卸载与重载 (VRAM Offloading & Reloading)** 为了节省显存，系统采用了“分时复用”的策略。当一个模块（如生成模块）完成任务后，其占用的模型权重会被**自动卸载（Offload）**到 CPU 内存或磁盘中，从而释放 GPU 显存给下一个模块（如训练模块）使用。这使得在 GPU 资源有限的情况下训练超大模型成为可能。
- **流水线与并行 (Pipeline & Parallelism)** 系统不仅在模块内部使用了专家并行，还在模块间实现了重叠执行（Overlapped Execution）。例如，规则奖励的计算与其他 GPU 密集型任务并行处理，极大地提高了整体吞吐量。

### 总结与类比

DeepSeek-R1 的 RL 基础设施就像一个**高度专业化的流水线工厂**：

- **生成车间（Rollout）**：只负责快速生产大量产品（回答），使用了最快的机器（vLLM + MTP）。
- **质检车间（Inference & Rule-based）**：分为两组，一组用机器扫描（Reference/Reward Model），一组人工核对（Rule-based）。核对工作很慢，所以被安排在后台与其他工作同时进行（异步）。
- **研发车间（Training）**：根据质检报告改进工艺（更新参数）。
- **动态调度（VRAM Offloading）**：因为车间空间（显存）有限，无法同时容纳所有机器。所以，当“生成机器”用完后，会被立刻移出车间，给“训练机器”腾出位置，确保每一寸空间都被高效利用。



**核心目的：** **巩固推理能力**并**扩展通用能力**。

在冷启动和第一阶段 RL 之后，模型虽然懂了规矩且推理变强了，但还不够全面。这 800k 数据（具体分为约 60万推理数据 + 20万非推理数据）主要为了解决以下问题：

A. 提纯与巩固推理能力 (600k Reasoning Data)

• **拒绝采样 (Rejection Sampling)：** 团队使用已经变强的模型（Checkpoint）对各种题目生成大量的思维链答案。然后通过验证器（如代码编译器、数学答案匹配）筛选出**正确**的答案7。

• **过滤低质内容：** 去掉那些虽然答案对，但思维过程混乱、混合语言的样本7。

• **作用：** 这相当于把模型在强化学习中偶尔灵光一现的“最佳解题路径”固定下来，通过大规模的 SFT 刻入模型的权重中，让其推理能力更加稳定。

B. 补齐通用能力短板 (200k Non-Reasoning Data)

• **防止偏科：** 之前的训练太专注于数学和代码，导致模型在**写作、事实问答、翻译、自我认知**等通用任务上能力可能退化。

• **数据复用：** 这部分数据主要复用了 DeepSeek-V3 的高质量 SFT 数据8。

• **作用：** 确保 DeepSeek-R1 不仅仅是一个“做题家”，还是一个能写诗、能聊天、能干活的通用 AI 助手9。

根据 DeepSeek-R1 的技术报告，其评测过程非常详尽，涵盖了从基础能力到推理专精能力的多个维度，并包含了一份深入的安全性分析报告。以下是对 DeepSeek-R1 评测体系、实验方法、核心结果及安全报告的详细解读。

### 1. 评测设置与实验方法 (Experiment Setup)

DeepSeek 团队采取了严格的评测标准，以确保结果的真实性和可比性：

- **基准测试集 (Benchmarks)：** 涵盖了通用知识（MMLU, MMLU-Pro）、数学（AIME 2024, MATH-500）、代码（LiveCodeBench, Codeforces）、长文本（FRAMES）以及指令遵循（IFEval）等多个领域。
- **对比基线 (Baselines)：** 主要对比了业内最顶尖的模型，包括 **OpenAI-o1-1217**、**GPT-4o-0513**、**Claude-3.5-Sonnet** 以及自家的 DeepSeek-V3。
- **评测指标 (Metrics)：**
    - **Pass@k 机制：** 针对推理模型长输出容易出现重复的问题，报告指出贪婪解码（Greedy Decoding）会导致较大的方差。因此，实验主要采用非零温度（Temperature=0.6, Top-p=0.95）下的 **Pass@1** 作为主要指标。对于 AIME 等数学竞赛，还使用了 **Consensus@64**（64次采样后的多数投票）来评估上限。
    - **去污染 (Decontamination)：** 为了防止“背题”，团队对训练数据进行了严格的 N-gram 过滤，剔除了与测试集（如 AIME 2024）重叠的内容。

### 2. 核心实验结果 (Key Results)

DeepSeek-R1 在推理任务上取得了与 OpenAI o1 正面对抗的成绩，在部分领域甚至实现了超越。

- **数学与推理能力：**
    - **AIME 2024：** DeepSeek-R1 的 Pass@1 达到了 **79.8%**，与 OpenAI-o1-1217 (79.2%) 持平，远超 GPT-4o (9.3%)。
    - **MATH-500：** 得分高达 **97.3%**，不仅超过了 GPT-4o，也略高于 OpenAI-o1-1217 (96.4%)。
    - **超过人类平均水平：** 在 AIME 和 Codeforces 等竞赛中，DeepSeek-R1 的表现均超越了人类参赛者的平均水平。
- **代码能力：**
    - **Codeforces：** 达到了 **96.3 百分位**（Elo 2029），表现极具统治力。
    - **工程类代码：** 在 SWE-Verified 上得分为 49.2%，与 Claude-3.5-Sonnet (50.8%) 相当，表明其不仅擅长算法题，也能处理实用的软件工程问题。
- **通用能力与人类偏好：**
    - **MMLU：** 得分 **90.8%**，证明其在通用百科知识上依然保持了顶级水准。
    - **Chatbot Arena：** 在“风格控制”榜单上，DeepSeek-R1 与 OpenAI-o1 并列 **总榜第一**，这意味着其回答不仅准确，而且深受人类用户喜爱。
- **蒸馏模型 (Distilled Models) 的惊人表现：**
    - 实验结果显示，通过蒸馏 DeepSeek-R1 的输出训练出来的小模型表现惊人。例如，**DeepSeek-R1-Distill-Qwen-1.5B** 在 MATH 基准上的得分（83.9%）竟然超过了庞大的 GPT-4o (74.6%) 和 Claude-3.5-Sonnet (78.3%)。这证明了推理能力是可以被高效“压缩”并传授给小模型的。

### 3. 安全报告 (Safety Report)

DeepSeek 在报告中坦诚地展示了模型的安全风险评估，采用了“红队测试”和内部基准测试相结合的方式。

- **风险控制系统 (Risk Control System)：**
    - DeepSeek-R1 的服务部署了一个**外挂的风险控制系统**。该系统会检测用户输入，若发现潜在风险，会将其通过一个专门设计的 Prompt 发送给 DeepSeek-V3 进行审查。
- **安全评测结果：**
    - **带防御 vs. 不带防御：** 报告对比了模型“裸奔”（无外挂防御）和“带防御”的数据。结果显示，**带有风险控制系统的 DeepSeek-R1 安全性处于第一梯队**，与 Claude-3.7-Sonnet 相当。
    - **越狱攻击 (Jailbreak)：** 实验发现，**推理模型（如 R1 和 o1）在没有外部防御时，比普通模型更容易遭受“越狱”攻击**（R1 的越狱成功率高达 85.9%）。这是因为推理模型倾向于遵循指令进行深度思考，容易被复杂的提示词诱导。但在开启风险控制系统后，越狱成功率骤降至 4.3%，安全性大幅提升。
- **HarmBench 的特殊情况：**
    - 在 HarmBench 测试中，R1 的分数看似较低，但分析指出这是因为 R1 未能拒绝诸如“生成《What a Wonderful World》歌词”这类涉及版权的请求，而非生成了有害内容。

### 总结

DeepSeek-R1 的评测结果传达了两个关键信息：

1. **能力验证：** 纯强化学习不仅能让模型在数学和代码上达到 SOTA（最先进）水平，还能通过蒸馏大幅提升小模型的上限。
2. **安全策略：** 推理模型本身可能因为“太听话”而容易被利用（越狱），因此**“模型能力”与“安全防御”的解耦**（即外挂风控系统）是部署推理模型的必要手段。