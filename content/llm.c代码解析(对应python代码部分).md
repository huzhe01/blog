
# Python å®ç° GPT-2 è®­ç»ƒï¼ˆå¯¹ç…§ train_gpt2.cï¼‰


æœ¬æ–‡æ¡£è¯¦ç»†å¯¹æ¯” **train_gpt2.c** å’Œ **Python å®ç°**ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚

---


## ğŸ“¦ æ–‡ä»¶è¯´æ˜

æˆ‘ä¸ºä½ åˆ›å»ºäº†ä¸¤ä¸ª Python å®ç°ç‰ˆæœ¬ï¼š
  

1. **train_gpt2_simple.py** - ä½¿ç”¨ PyTorchï¼ˆæ¨èï¼Œè‡ªåŠ¨å¾®åˆ†ï¼‰

2. **train_gpt2_numpy.py** - ä½¿ç”¨çº¯ NumPyï¼ˆæ•™è‚²æ€§å¼ºï¼Œæ‰‹åŠ¨å®ç°æ‰€æœ‰å¾ªç¯ï¼‰

---
## ğŸ”§ ç¯å¢ƒå‡†å¤‡

### å®‰è£…ä¾èµ–

```bash

# å®‰è£… PyTorch ç‰ˆæœ¬ï¼ˆæ¨èï¼‰

pip install torch numpy

  

# æˆ–è€…åªå®‰è£… NumPyï¼ˆçº¯ NumPy ç‰ˆæœ¬ï¼‰

pip install numpy

```

  

### è¿è¡Œä»£ç 

  

```bash

# PyTorch ç‰ˆæœ¬

python3 train_gpt2_simple.py

  

# NumPy ç‰ˆæœ¬ï¼ˆè¾ƒæ…¢ä½†æ•™è‚²æ€§å¼ºï¼‰

python3 train_gpt2_numpy.py

```

  

---

  

## ğŸ“Š ä»£ç å¯¹æ¯”ï¼šC vs Python

  

### 1ï¸âƒ£ **é…ç½®ç»“æ„**

  

#### C ä»£ç 

```c

typedef struct {

int max_seq_len;

int vocab_size;

int padded_vocab_size;

int num_layers;

int num_heads;

int channels;

} GPT2Config;

```

  

#### Python (PyTorch)

```python

@dataclass

class GPT2Config:

max_seq_len: int = 1024

vocab_size: int = 50257

padded_vocab_size: int = 50304

num_layers: int = 12

num_heads: int = 12

channels: int = 768

```

  

---

  

### 2ï¸âƒ£ **Encoderï¼ˆToken + ä½ç½®åµŒå…¥ï¼‰**

  

#### C ä»£ç 

```c

void encoder_forward(float* out, int* inp, float* wte, float* wpe,

int B, int T, int C) {

for (int b = 0; b < B; b++) {

for (int t = 0; t < T; t++) {

float* out_bt = out + b * T * C + t * C;

int ix = inp[b * T + t];

float* wte_ix = wte + ix * C;

float* wpe_t = wpe + t * C;

for (int i = 0; i < C; i++) {

out_bt[i] = wte_ix[i] + wpe_t[i];

}

}

}

}

```

  

#### Python (PyTorch) - å‘é‡åŒ–

```python

def encoder_forward(inp: torch.Tensor, wte: torch.Tensor, wpe: torch.Tensor):

"""

inp: (B, T) token IDs

wte: (V, C) token embeddings

wpe: (maxT, C) position embeddings

è¿”å›: (B, T, C)

"""

B, T = inp.shape

token_embeddings = wte[inp] # (B, T, C)

positions = torch.arange(T, device=inp.device)

position_embeddings = wpe[positions] # (T, C)

return token_embeddings + position_embeddings # å¹¿æ’­

```

  

#### Python (NumPy) - æ˜¾å¼å¾ªç¯

```python

def encoder_forward(inp: np.ndarray, wte: np.ndarray, wpe: np.ndarray):

B, T = inp.shape

C = wte.shape[1]

out = np.zeros((B, T, C), dtype=np.float32)

# ä¸ C ä»£ç å®Œå…¨å¯¹åº”çš„å¾ªç¯

for b in range(B):

for t in range(T):

ix = inp[b, t]

out[b, t, :] = wte[ix, :] + wpe[t, :]

return out

```

  

**å¯¹åº”å…³ç³»ï¼š**

- C çš„æŒ‡é’ˆåç§» â†’ Python çš„æ•°ç»„ç´¢å¼•

- C çš„ä¸‰å±‚å¾ªç¯ â†’ PyTorch çš„å‘é‡åŒ– / NumPy çš„æ˜¾å¼å¾ªç¯

  

---

  

### 3ï¸âƒ£ **LayerNormï¼ˆå±‚å½’ä¸€åŒ–ï¼‰**

  

#### C ä»£ç 

```c

void layernorm_forward(float* out, float* mean, float* rstd,

float* inp, float* weight, float* bias,

int B, int T, int C) {

float eps = 1e-5f;

for (int b = 0; b < B; b++) {

for (int t = 0; t < T; t++) {

float* x = inp + b * T * C + t * C;

// è®¡ç®—å‡å€¼

float m = 0.0f;

for (int i = 0; i < C; i++) {

m += x[i];

}

m = m/C;

// è®¡ç®—æ–¹å·®

float v = 0.0f;

for (int i = 0; i < C; i++) {

float xshift = x[i] - m;

v += xshift * xshift;

}

v = v/C;

// å½’ä¸€åŒ–

float s = 1.0f / sqrtf(v + eps);

float* out_bt = out + b * T * C + t * C;

for (int i = 0; i < C; i++) {

float n = (s * (x[i] - m));

float o = n * weight[i] + bias[i];

out_bt[i] = o;

}

mean[b * T + t] = m;

rstd[b * T + t] = s;

}

}

}

```

  

#### Python (PyTorch)

```python

def layernorm_forward(inp, weight, bias, eps=1e-5):

"""inp: (B, T, C)"""

mean = inp.mean(dim=-1, keepdim=True) # (B, T, 1)

var = inp.var(dim=-1, keepdim=True, unbiased=False) # (B, T, 1)

rstd = 1.0 / torch.sqrt(var + eps) # (B, T, 1)

norm = (inp - mean) * rstd # (B, T, C)

out = norm * weight + bias # (B, T, C)

return out, mean.squeeze(-1), rstd.squeeze(-1)

```

  

#### Python (NumPy)

```python

def layernorm_forward(inp, weight, bias, eps=1e-5):

B, T, C = inp.shape

mean = np.mean(inp, axis=-1) # (B, T)

var = np.var(inp, axis=-1) # (B, T)

rstd = 1.0 / np.sqrt(var + eps) # (B, T)

out = np.zeros_like(inp)

# ä¸ C ä»£ç å®Œå…¨å¯¹åº”çš„å¾ªç¯

for b in range(B):

for t in range(T):

norm = (inp[b, t, :] - mean[b, t]) * rstd[b, t]

out[b, t, :] = norm * weight + bias

return out, mean, rstd

```

  

---

  

### 4ï¸âƒ£ **Attentionï¼ˆè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼‰**

#### C ä»£ç ï¼ˆç®€åŒ–ï¼‰

```c

void attention_forward(float* out, float* preatt, float* att,

float* inp, int B, int T, int C, int NH) {

int hs = C / NH;

float scale = 1.0 / sqrtf(hs);

for (int b = 0; b < B; b++) {

for (int t = 0; t < T; t++) {

for (int h = 0; h < NH; h++) {

// Pass 1: Q @ K

for (int t2 = 0; t2 <= t; t2++) {

float val = 0.0f;

for (int i = 0; i < hs; i++) {

val += query_t[i] * key_t2[i];

}

preatt[t2] = val * scale;

}

// Pass 2 & 3: Softmax

float maxval = max(preatt);

float expsum = sum(exp(preatt - maxval));

for (int t2 = 0; t2 <= t; t2++) {

att[t2] = exp(preatt[t2] - maxval) / expsum;

}

// Pass 4: att @ V

for (int t2 = 0; t2 <= t; t2++) {

for (int i = 0; i < hs; i++) {

out[i] += att[t2] * value_t2[i];

}

}

}

}

}

}

```

  

#### Python (PyTorch) - ä½¿ç”¨ scaled_dot_product_attention

```python

def attention_forward(inp, B, T, C, NH):

"""inp: (B, T, 3*C) åŒ…å« Q, K, V"""

qkv = inp.view(B, T, 3, C)

q, k, v = qkv[:, :, 0], qkv[:, :, 1], qkv[:, :, 2]

hs = C // NH

q = q.view(B, T, NH, hs).transpose(1, 2) # (B, NH, T, hs)

k = k.view(B, T, NH, hs).transpose(1, 2)

v = v.view(B, T, NH, hs).transpose(1, 2)

# è®¡ç®—æ³¨æ„åŠ›

scale = 1.0 / np.sqrt(hs)

att = (q @ k.transpose(-2, -1)) * scale # (B, NH, T, T)

# å› æœæ©ç 

mask = torch.tril(torch.ones(T, T))

att = att.masked_fill(mask == 0, float('-inf'))

# Softmax + åŠ æƒæ±‚å’Œ

att = F.softmax(att, dim=-1)

out = att @ v # (B, NH, T, hs)

return out.transpose(1, 2).contiguous().view(B, T, C)

```

  

---

  

### 5ï¸âƒ£ **å®Œæ•´çš„å‰å‘ä¼ æ’­**

  

#### C ä»£ç 

```c

void gpt2_forward(GPT2 *model, int* inputs, int* targets, size_t B, size_t T) {

// 1. Embedding

encoder_forward(acts.encoded, inputs, params.wte, params.wpe, B, T, C);

// 2. Transformer å±‚

for (int l = 0; l < L; l++) {

layernorm_forward(l_ln1, ...);

matmul_forward(l_qkv, ...);

attention_forward(l_atty, ...);

matmul_forward(l_attproj, ...);

residual_forward(l_residual2, ...);

layernorm_forward(l_ln2, ...);

matmul_forward(l_fch, ...);

gelu_forward(l_fch_gelu, ...);

matmul_forward(l_fcproj, ...);

residual_forward(l_residual3, ...);

}

// 3. è¾“å‡ºå±‚

layernorm_forward(acts.lnf, ...);

matmul_forward(acts.logits, ...);

softmax_forward(acts.probs, ...);

// 4. æŸå¤±

if (targets != NULL) {

crossentropy_forward(model->acts.losses, ...);

}

}

```

  

#### Python (PyTorch)

```python

class GPT2(nn.Module):

def forward(self, inputs, targets=None):

B, T = inputs.shape

# 1. Embedding

x = encoder_forward(inputs, self.wte, self.wpe)

# 2. Transformer å±‚

residual = x

for l in range(self.config.num_layers):

# Pre-LN + Attention

ln1_out, _, _ = layernorm_forward(residual, self.ln1w[l], self.ln1b[l])

qkv = matmul_forward(ln1_out, self.qkvw[l], self.qkvb[l])

atty = attention_forward(qkv, B, T, C, NH)

attproj = matmul_forward(atty, self.attprojw[l], self.attprojb[l])

residual2 = residual + attproj

# Pre-LN + MLP

ln2_out, _, _ = layernorm_forward(residual2, self.ln2w[l], self.ln2b[l])

fch = matmul_forward(ln2_out, self.fcw[l], self.fcb[l])

fch_gelu = gelu_forward(fch)

fcproj = matmul_forward(fch_gelu, self.fcprojw[l], self.fcprojb[l])

residual = residual2 + fcproj

# 3. è¾“å‡ºå±‚

lnf_out, _, _ = layernorm_forward(residual, self.lnfw, self.lnfb)

logits = matmul_forward(lnf_out, self.wte, None) # æƒé‡å…±äº«

# 4. æŸå¤±

loss = None

if targets is not None:

probs = softmax_forward(logits)

losses = crossentropy_forward(probs, targets)

loss = losses.mean()

return logits, loss

```

  

---

  

### 6ï¸âƒ£ **è®­ç»ƒå¾ªç¯**

  

#### C ä»£ç 

```c

int main() {

GPT2 model;

gpt2_build_from_checkpoint(&model, "gpt2_124M.bin");

for (int step = 0; step <= 40; step++) {

// è®­ç»ƒä¸€æ­¥

dataloader_next_batch(&train_loader);

gpt2_forward(&model, train_loader.inputs, train_loader.targets, B, T);

gpt2_zero_grad(&model);

gpt2_backward(&model);

gpt2_update(&model, 1e-4f, 0.9f, 0.999f, 1e-8f, 0.0f, step+1);

printf("step %d: train loss %f\n", step, model.mean_loss);

}

}

```

  

#### Python (PyTorch)

```python

def main():

config = GPT2Config(...)

model = GPT2(config)

optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

for step in range(40):

# ç”Ÿæˆ/åŠ è½½æ•°æ®

inputs = ...

targets = ...

# å‰å‘ä¼ æ’­

logits, loss = model(inputs, targets)

# åå‘ä¼ æ’­

optimizer.zero_grad()

loss.backward() # PyTorch è‡ªåŠ¨æ±‚å¯¼ï¼

optimizer.step()

print(f"step {step}: train loss {loss.item():.6f}")

```

  

---

  

## ğŸ¯ å…³é”®å¯¹åº”å…³ç³»æ€»ç»“

  

| C ä»£ç  | Python (PyTorch) | Python (NumPy) |

|--------|-----------------|----------------|

| `float*` æŒ‡é’ˆ | `torch.Tensor` | `np.ndarray` |

| æ‰‹åŠ¨å†…å­˜ç®¡ç† | è‡ªåŠ¨å†…å­˜ç®¡ç† | è‡ªåŠ¨å†…å­˜ç®¡ç† |

| å¤šå±‚ for å¾ªç¯ | å‘é‡åŒ–æ“ä½œ | æ˜¾å¼ for å¾ªç¯ |

| æ‰‹å†™æ¢¯åº¦è®¡ç®— | `loss.backward()` | éœ€æ‰‹å†™ï¼ˆæœªå®ç°ï¼‰ |

| `malloc/free` | è‡ªåŠ¨åƒåœ¾å›æ”¶ | è‡ªåŠ¨åƒåœ¾å›æ”¶ |

| OpenMP å¹¶è¡Œ | CUDA å¹¶è¡Œ | å•çº¿ç¨‹ |

  

---

  

## ğŸš€ ä¼˜ç¼ºç‚¹å¯¹æ¯”

  

### C ç‰ˆæœ¬ï¼ˆtrain_gpt2.cï¼‰

âœ… **ä¼˜ç‚¹ï¼š**

- å®Œå…¨æ§åˆ¶å†…å­˜å’Œæ€§èƒ½

- æ•™è‚²æ€§å¼ºï¼Œæ¯æ­¥éƒ½æ¸…æ™°å¯è§

- æ— ä¾èµ–ï¼Œå¯ç§»æ¤æ€§å¼º

  

âŒ **ç¼ºç‚¹ï¼š**

- ä»£ç é‡å¤§ï¼Œéœ€æ‰‹å†™æ‰€æœ‰æ“ä½œ

- æ‰‹åŠ¨ç®¡ç†å†…å­˜å®¹æ˜“å‡ºé”™

- è°ƒè¯•å›°éš¾

  

### Python + PyTorch

âœ… **ä¼˜ç‚¹ï¼š**

- ä»£ç ç®€æ´ï¼Œæ˜“äºå®éªŒ

- è‡ªåŠ¨æ±‚å¯¼ï¼Œæ— éœ€æ‰‹å†™æ¢¯åº¦

- GPU åŠ é€Ÿå¼€ç®±å³ç”¨

- ä¸°å¯Œçš„ç”Ÿæ€ç³»ç»Ÿ

  

âŒ **ç¼ºç‚¹ï¼š**

- é»‘ç›’æ“ä½œï¼Œä¸åˆ©äºç†è§£åº•å±‚

- ä¾èµ–åºå¤§ï¼ˆPyTorch ~1GBï¼‰

- æ€§èƒ½ä¸å¦‚æ‰‹å·¥ä¼˜åŒ–çš„ C/CUDA

  

### Python + NumPy

âœ… **ä¼˜ç‚¹ï¼š**

- ä¸ C ä»£ç ç»“æ„ä¸€ä¸€å¯¹åº”

- æ•™è‚²æ€§å¼ºï¼Œæ˜“äºç†è§£

- ä¾èµ–å°ï¼Œåªéœ€ NumPy

  

âŒ **ç¼ºç‚¹ï¼š**

- é€Ÿåº¦æ…¢ï¼ˆçº¯ CPUï¼Œæ— å¹¶è¡Œï¼‰

- ä¸é€‚åˆå®é™…è®­ç»ƒ

  

---

  

## ğŸ“ å­¦ä¹ å»ºè®®

  

1. **å…ˆè¯» C ä»£ç ** - ç†è§£æ¯ä¸ªæ“ä½œçš„åº•å±‚å®ç°

2. **å¯¹ç…§ NumPy ç‰ˆæœ¬** - çœ‹ Python å¦‚ä½•æ˜ å°„ C çš„å¾ªç¯ç»“æ„

3. **å­¦ä¹  PyTorch ç‰ˆæœ¬** - ç†è§£å¦‚ä½•ç”¨é«˜çº§ API å®ç°åŒæ ·çš„åŠŸèƒ½

4. **å®éªŒä¿®æ”¹** - å°è¯•æ”¹å˜æ¨¡å‹ç»“æ„ï¼Œè§‚å¯Ÿå˜åŒ–

  

---

  

## ğŸ”— è¿è¡Œç¤ºä¾‹

  

```bash

# å¦‚æœä½ çš„ç¯å¢ƒæ²¡æœ‰å®‰è£…ä¾èµ–ï¼Œå…ˆå®‰è£…ï¼š

pip install torch numpy

  

# è¿è¡Œ PyTorch ç‰ˆæœ¬ï¼ˆæ¨èï¼‰

python3 train_gpt2_simple.py

  

# è¿è¡Œ NumPy ç‰ˆæœ¬ï¼ˆæ•™è‚²æ€§å¼ºä½†è¾ƒæ…¢ï¼‰

python3 train_gpt2_numpy.py

  

# å¯¹æ¯” C ç‰ˆæœ¬

make train_gpt2

OMP_NUM_THREADS=8 ./train_gpt2

```

  

ç°åœ¨ä½ æœ‰äº†ä¸‰ä¸ªç‰ˆæœ¬çš„ä»£ç å¯ä»¥å¯¹æ¯”å­¦ä¹ ï¼ğŸ‰